{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ysZvQ69PFfX",
        "outputId": "cd6bb0ef-f6d5-46bd-82c0-e6c16ee4a47d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=db4b2440ef1aaa97972fd413d160ebd7a2ab950c16aae031c177848046b3baf3\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLj38aADPOdJ",
        "outputId": "faf31c5e-d806-4f2b-e474-4c71cb2d6da3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=519bb0ef2810f0ad88608096e0029e6b7e99a0fd33c7ecaa564a9a5037dfeb05\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resilient Distributed Dataset (RDD) is the fundamental building block of spark. It is an immutable distributed collection of objects. All the Data frame or Dataset are stored as RDD under the hood.  \n",
        "\n",
        "A dataset in Spark is called as RDD because:\n",
        "\n",
        "Resilient:  In case of any node in the cluster goes down, Spark can recover the parts of the RDDs from the input and pick up from where it left off because data is available in memory of other nodes. RDD can be reconstructed from the data available in memory of other node, that is why it is called as resilient.\n",
        "\n",
        "Distributed: Each RDD is broken into multiple pieces called partitions, and these partitions are divided across the clusters. This partitioning process is done automatically by Spark, so you don’t need to worry about all the details about how your data is partitioned across the cluster.\n",
        "\n",
        "Immutable: They cannot be changed after they are created. Immutability rules out a significant set of potential problems due to updates from multiple threads at once. RDD are immutable, similar to data frames that are built on top of RDD. You cannot change a RDD, but you can apply transformation and create a new RDD.\n",
        "\n"
      ],
      "metadata": {
        "id": "MO8nZld0_p-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a RDD\n",
        "Load RDDs from external storage: This approach is recommended for creating RDD from large datasets. This is done by executing textFile() function on  SparkContext. The external storage is usually a distributed file system from different HDFS and data sources. The user can upload any csv file and read the csv file by specifying the path in the function textFile() for spark context object. In the following example, the big data is fetched directly from online sources using wget library.\n",
        "\n"
      ],
      "metadata": {
        "id": "GHAf5JMEP2CX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulIVlCdAOplf",
        "outputId": "496c2407-8fa8-44a9-a7ec-c34fade46787"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type: <class 'pyspark.rdd.RDD'>\n"
          ]
        }
      ],
      "source": [
        "#Downloading the dataset using wget\n",
        "import wget\n",
        "wget.download(\"https://github.com/futurexskill/bigdata/raw/master/retailstore_large.zip\",\"retail.zip\")\n",
        "\n",
        "#Extracting the contents from the zip file in retail directory\n",
        "from zipfile import ZipFile\n",
        "with ZipFile('retail.zip','r') as file:\n",
        "     file.extractall('retaildir')\n",
        "\n",
        "#Creating a spark object\n",
        "from pyspark import SparkConf, SparkContext\n",
        "if __name__ == \"__main__\":\n",
        "    conf=SparkConf().setAppName(\"first\").setMaster(\"local\")\n",
        "    sc=SparkContext(conf=conf)\n",
        "\n",
        "#Creating a RDD from csv file\n",
        "RetailRDD=sc.textFile(\"retaildir/retailstore_large.csv\")\n",
        "\n",
        "#Displaying the type\n",
        "print(\"Type:\",type(RetailRDD))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: If set master has the value as local[2],  two processes will be executed for doing the computation. Both the processes first split the data and then reduce by join addition. The final result is the total of both the processes. However, it should be noted that if it is executed on the cluster, two machines will be deployed.\n"
      ],
      "metadata": {
        "id": "h0GnrPr7RsL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actions for RDD\n",
        "Two operations can b executed on RDD: Actions and Transformations. Actions are operations that return a result to the driver program or write it to storage, and kick off a computation. Transformations return RDDs, whereas actions return some other data type. Some of the most common actions include reduce(), collect(), first(), take(), count(), countbyvalue(), top(), saveAsTextFile() etc.  It should be noted that actions will also force the evaluation of the transformations required for the RDD they were called on. Even though new RDDS can be defined any time, they are only computed by Spark in a lazy fashion.Transformations also return the output in the form of RDD and are lazily evaluated, meaning that Spark will not begin to execute until it sees an action"
      ],
      "metadata": {
        "id": "sM4v1z5APjm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Count action helps to count number of rows in an RDD\n",
        "print(\"Number of rows:\",RetailRDD.count())\n",
        "\n",
        "#First action displays the first row\n",
        "print(\"Header of RDD:\\n\",RetailRDD.first())\n",
        "\n",
        "#Take action displays the specified number of elements from an RDD\n",
        "print(\"First 2 rows of RDD:\\n\",RetailRDD.take(2))\n",
        "\n",
        "#Top Action for fetching rows on sorted basis of length of row\n",
        "print(\"Sorted 4 rows of RDD:\\n\",RetailRDD.top(4))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWffQ1bLPSAv",
        "outputId": "4f924e51-c260-4e6d-b9a7-43994fe46f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows: 1048576\n",
            "Header of RDD:\n",
            " CustomerID,Age,Salary,Gender,Country\n",
            "First 2 rows of RDD:\n",
            " ['CustomerID,Age,Salary,Gender,Country', '1,18,20000,Male,Germany']\n",
            "Sorted 4 rows of RDD:\n",
            " ['CustomerID,Age,Salary,Gender,Country', '999999,51.72727273,12600,Male,France', '999998,50.34545455,34000,Male,France', '999997,48.96363636,32000,Male,France']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to transform the dataset before applying advanced analysis. This can be done using traditional technologies also but, Spark is used for doing transformation of high volume of data and when it is required to do high level of processing. Transformations and actions are different because of the way how spark computes RDDs. Transformations on RDDs are lazily evaluated, meaning that Spark will not begin to execute until it sees an action.Transformations on RDD:Some of the most common transformations include map(),  mapValues(), mapPartitions(), filter(),  flatMap(), sample(),  randomSplit(), distinct(), coalesce() and repartition() etc."
      ],
      "metadata": {
        "id": "rDzxEBlLQoUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Map Transformation accepts input parameter as a function. The function can be any user defined function or one line lambda function.\n",
        "#Using lambda function for transformation\n",
        "CountryRDD=RetailRDD.map(lambda x: x.split(\",\")[4])\n",
        "print(CountryRDD.take(2))\n",
        "\n",
        "#Using Count by Value Action on a RDD\n",
        "print(\"Using Count by value :\",)\n",
        "result=CountryRDD.countByValue()\n",
        "print(result.items())\n",
        "for word,count in result.items():\n",
        "    print(word, \" occurred \", count, \" times\")\n",
        "\n",
        "#saveAsTextFile is used to write on distributed storage system (HDFS or local system)\n",
        "CountryRDD.saveAsTextFile(\"newRetail4.csv\")\n",
        "print(\"File saved\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvI6EkSlE2yT",
        "outputId": "f062d76e-35e4-4e03-d456-6c23c4078285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Country', 'Germany']\n",
            "Using Count by value :\n",
            "dict_items([('Country', 1), ('Germany', 278528), ('France', 294911), ('England', 475136)])\n",
            "Country  occurred  1  times\n",
            "Germany  occurred  278528  times\n",
            "France  occurred  294911  times\n",
            "England  occurred  475136  times\n",
            "File saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Practical Exercise:  Determine the number of male and female customers"
      ],
      "metadata": {
        "id": "Tu0JFDXXmpFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Map transformation with user defined function on RDD created from csv\n",
        "#Creating a user defined function\n",
        "def retail_func(myrecord):\n",
        "    id=myrecord.split(\",\")[0]\n",
        "    salary=myrecord.split(\",\")[2]\n",
        "    country=myrecord.split(\",\")[4]\n",
        "    return \"Salary of \"+ id +\" is \"+ salary+ \" and belongs to \"+country\n",
        "\n",
        "#Map transformation with user defined function\n",
        "mapRDD=RetailRDD.map(retail_func)\n",
        "print(mapRDD.take(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55VfSXeYQTxA",
        "outputId": "41e9432a-f725-4152-d515-c1d6238e2a05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Salary of CustomerID is Salary and belongs to Country', 'Salary of 1 is 20000 and belongs to Germany', 'Salary of 2 is 22000 and belongs to France']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Filter transformation for filtering rows\n",
        "#Using filter for filtering the header\n",
        "header=RetailRDD.first()\n",
        "FinalRDD=RetailRDD.filter(lambda x: x!=header)\n",
        "print(\"First few rows:\\n\",FinalRDD.take(2))\n",
        "\n",
        "#Using 'in' for filtering categorical variable\n",
        "#Observe the difference between two results.\n",
        "#Approach 1: When the string is directly searched\n",
        "FranceRDD=FinalRDD.filter(lambda x:'France' in x)\n",
        "print(\"Number of rows containing France:\",FranceRDD.count())\n",
        "print(\"First few rows:\\n\",FranceRDD.take(2))\n",
        "\n",
        "\n",
        "#Approach 2: When the string is directly searched from the filtered RDD\n",
        "CountryRDD=FinalRDD.map(lambda x: x.split(\",\")[4])\n",
        "GermanyRDD=CountryRDD.filter(lambda x:'Germany' in x)\n",
        "print(\"Number of rows containing Germany:\",GermanyRDD.count())\n",
        "print(\"First few rows:\\n\",GermanyRDD.take(2))\n",
        "\n",
        "\n",
        "#Using filter for numeric variable\n",
        "SalaryRDD=FinalRDD.map(lambda x: x.split(\",\")[2])\n",
        "ResultRDD=SalaryRDD.filter(lambda x:int(x)>60000)\n",
        "print(\"Number of rows where salary > 60000:\",ResultRDD.count())\n",
        "print(\"First few rows:\\n\",ResultRDD.take(2))\n",
        "\n",
        "#Using 'and' Logical operators for filtering multiple conditions\n",
        "LogicalRDD1=SalaryRDD.filter(lambda x:(int(x)>40000) & (int(x)< 50000))\n",
        "print(\"Number of rows between 40K and 50K:\",LogicalRDD1.count())\n",
        "print(\"First few rows:\\n\",LogicalRDD1.take(2))\n",
        "\n",
        "#Using 'or' Logical operators for filtering multiple conditions\n",
        "LogicalRDD2=SalaryRDD.filter(lambda x:(int(x)>50000)|(int(x) < 20000))\n",
        "print(\"Number of rows with high salary:\",LogicalRDD2.count())\n",
        "print(\"First few rows:\\n\",LogicalRDD2.take(2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcO2GgvBR-xD",
        "outputId": "a5c29d42-4d47-4453-f472-db51420fb1f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows:\n",
            " ['1,18,20000,Male,Germany', '2,19,22000,Female,France']\n",
            "Number of rows containing France: 294911\n",
            "First few rows:\n",
            " ['2,19,22000,Female,France', '5,22,50000,Male,France']\n",
            "Number of rows containing Germany: 278528\n",
            "First few rows:\n",
            " ['Germany', 'Germany']\n",
            "Number of rows where salary > 60000: 41942\n",
            "First few rows:\n",
            " ['65000', '65000']\n",
            "Number of rows between 40K and 50K: 209711\n",
            "First few rows:\n",
            " ['42000', '45000']\n",
            "Number of rows with high salary: 377485\n",
            "First few rows:\n",
            " ['2600', '4300']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using multiple filters for multiple categorical, continuous variables\n",
        "MaleRDD=FinalRDD.filter(lambda x: 'Male' in x)\n",
        "print(\"Number of male customers:\", MaleRDD.count())\n",
        "print(\"First few rows:\\n\",MaleRDD.take(2))\n",
        "\n",
        "MaleEngRDD=MaleRDD.filter(lambda x: 'England' in x)\n",
        "print(\"Number of male customers from England:\",MaleEngRDD.count())\n",
        "print(\"First few rows:\\n\",MaleEngRDD.take(2))\n",
        "\n",
        "MaleEngSalRDD=MaleEngRDD.map(lambda x: x.split(\",\")[2])\n",
        "MaleEngSalHighRDD=MaleEngSalRDD.filter(lambda x:int(x)>50000)\n",
        "print(\"Number of male customers from England having salary>50000:\", MaleEngSalHighRDD.count())\n",
        "print(\"First few rows:\\n\",MaleEngSalHighRDD.take(2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv1MigrwSyAn",
        "outputId": "a97c57e6-159c-47f7-9460-d8bfc3cea796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of male customers: 544195\n",
            "First few rows:\n",
            " ['1,18,20000,Male,Germany', '4,21,2600,Male,England']\n",
            "Number of male customers from England: 246591\n",
            "First few rows:\n",
            " ['4,21,2600,Male,England', '14,35.14545455,7600,Male,England']\n",
            "Number of male customers from England having salary>50000: 29594\n",
            "First few rows:\n",
            " ['65000', '65000']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using and operation for searching two strings together\n",
        "MaleEngRDD2=FinalRDD.filter(lambda x: ('Male' in x) & ('England' in x))\n",
        "print(\"Number of male customers:\", MaleEngRDD2.count())\n",
        "print(\"First few rows:\\n\",MaleEngRDD2.take(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51PUZc2Nn9cv",
        "outputId": "7789cc96-7b7b-4a36-c903-0cf45de91b2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of male customers: 246591\n",
            "First few rows:\n",
            " ['4,21,2600,Male,England', '14,35.14545455,7600,Male,England']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exercise: Determine the number of female customers from France where age is greater than 40"
      ],
      "metadata": {
        "id": "QxPnXjtQj3y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exercise: Search for multiple conditions: Display the information of\n",
        "#country, age and salary of records where age > 30 and salary > 40000\n",
        "#Create a user defined function\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "POK8Ih6QpboB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Determining Total salary using map transformation and reduce action\n",
        "SalaryRDD=FinalRDD.map(lambda x: x.split(\",\")[2])\n",
        "Total_Salary=SalaryRDD.reduce(lambda x,y: int(x)+int(y))\n",
        "print(\"Total salary:\",Total_Salary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aZBuFVCTE2z",
        "outputId": "81465115-3e6c-4699-af12-a285e80a2dc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total salary: 37106829800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exercise: Determine the maximum age of male customers from Germany\n",
        "\n"
      ],
      "metadata": {
        "id": "mDiTcWBukuIL"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample Transformation\n",
        "SampleRDD=FinalRDD.sample(withReplacement=False,fraction=0.00001,seed=1)\n",
        "print(\"Number of rows in sample:\",SampleRDD.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM9QDKcBc9Cz",
        "outputId": "c74a06fb-d9b6-4f14-9919-0aa1a9d9bfe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in sample: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Flatmap transformation\n",
        "FlatRDD=SampleRDD.flatMap(lambda x:x.split(\",\"))\n",
        "print(\"Using FlatMap transformation for counting:\\n\",FlatRDD.count())\n",
        "print(\"Using FlatMap transformation:\\n\",FlatRDD.take(8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5SMyUuxds-z",
        "outputId": "0b838cc1-3f09-45ca-bedb-b77656f84c62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using FlatMap transformation for counting:\n",
            " 45\n",
            "Using FlatMap transformation:\n",
            " ['20964', '55.87272727', '7600', 'Male', 'England', '45816', '47.58181818', '40000']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Random Split Transformation\n",
        "RandomRDD=FinalRDD.randomSplit(weights=[0.2,0.4,0.3,0.1], seed=1000)\n",
        "print(\"Rows in first RDD:\",RandomRDD[0].count())\n",
        "print(\"Rows in second RDD:\",RandomRDD[1].count())\n",
        "print(\"Rows in third RDD:\",RandomRDD[2].count())\n",
        "print(\"Rows in fourth RDD:\",RandomRDD[3].count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iq1DR6ZcdKUf",
        "outputId": "65457010-7af3-4c47-bcb1-0530b9871551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows in first RDD: 209910\n",
            "Rows in second RDD: 418581\n",
            "Rows in third RDD: 315187\n",
            "Rows in fourth RDD: 104897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Distinct transformation\n",
        "GenderRDD=FinalRDD.map(lambda x: x.split(\",\")[3])\n",
        "print(\"Distinct values for gender:\",GenderRDD.distinct().collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFbY5nukdSZc",
        "outputId": "b3228f20-05ed-4618-d5fa-e41133f99fad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distinct values for gender: ['Male', 'Female']\n"
          ]
        }
      ]
    }
  ]
}